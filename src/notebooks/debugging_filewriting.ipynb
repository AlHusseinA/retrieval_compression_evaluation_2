{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alabutaleb/Desktop/myprojects/debugging_mvp/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_retrieval_Kam_Woh_topK_mAP(sorted_indices, query_labels, gallery_labels, dir_results, features_size, top_k=100):\n",
    "    # Basic input validation\n",
    "    assert isinstance(sorted_indices, torch.Tensor), \"sorted_indices must be a PyTorch Tensor\"\n",
    "    assert isinstance(query_labels, torch.Tensor), \"query_labels must be a PyTorch Tensor\"\n",
    "    assert isinstance(gallery_labels, torch.Tensor), \"gallery_labels must be a PyTorch Tensor\"\n",
    "    assert isinstance(top_k, int) and top_k > 0, \"top_k must be a positive integer\"\n",
    "    assert isinstance(dir_results, str), \"dir_results must be a string\"\n",
    "\n",
    "    # Ensure the lengths of the sorted_indices and query_labels match\n",
    "    assert sorted_indices.shape[0] == len(query_labels), \"Length of sorted_indices and query_labels must match\"\n",
    "\n",
    "    # Prepare directory to save results\n",
    "    if not os.path.exists(dir_results):\n",
    "        os.makedirs(dir_results)    \n",
    "\n",
    "    # if isinstance(features_size, str): # this will get triggered if we're evaluation the vanilla model since feature_size will contain the word 'vanilla'\n",
    "    #     print(f\"Parameter features_size a string {features_size}\")\n",
    "    #     print(f\"Retrieval results for feature size {features_size} will be saved to {dir_results}\\n\")\n",
    "    #     result_file_path = os.path.join(dir_results, f\"retrieval_results_size_VANILLA_topK_{top_k}.txt\")\n",
    "    #     # print(f\"the full dir+filename is: {result_file_path}\\n\")\n",
    "\n",
    "    # elif isinstance(features_size, int): # this will get triggerd if we're evaluation baseline models as feature_size will be the size of the feature layer\n",
    "    #     print(f\"Parameter features_size an integer {features_size}\")\n",
    "    #     print(f\"Retrieval results for feature size {features_size} will be saved to {dir_results}\\n\")\n",
    "    #     result_file_path = os.path.join(dir_results, f\"retrieval_results_size_{features_size}_topK_{top_k}.txt\")\n",
    "    #     # print(f\"the full dir+filename is: {result_file_path}\\n\")\n",
    "\n",
    "    # else:\n",
    "    #     raise ValueError(\"features_size must be a positive integer >=1\")\n",
    "\n",
    "\n",
    "    result_file_path = os.path.join(dir_results, f\"retrieval_results_size_{features_size}_topK_{top_k}.txt\")\n",
    "\n",
    "    APs = []\n",
    "    AP_top_ks = []\n",
    "    Rs = [1, 5, 10, 20]\n",
    "    recalls = {R: [] for R in Rs}\n",
    "    precisions = {R: [] for R in Rs}\n",
    "\n",
    "    # Set offset based on whether query & gallery are completely different\n",
    "    offset = 1  # Adjust as needed\n",
    "\n",
    "    for qi in tqdm(range(len(query_labels))):\n",
    "        query_label = query_labels[qi]\n",
    "\n",
    "        query_retrieved_indices_full = sorted_indices[qi][offset:]\n",
    "        retrieved_labels_full = gallery_labels[query_retrieved_indices_full]\n",
    "\n",
    "        # For mAP calculation, limit to top_k\n",
    "        query_retrieved_indices_top_k = sorted_indices[qi][offset:offset+top_k] \n",
    "        retrieved_labels_top_k = gallery_labels[query_retrieved_indices_top_k]\n",
    "\n",
    "        imatch_full = torch.eq(retrieved_labels_full, query_label)\n",
    "        imatch_top_k = torch.eq(retrieved_labels_top_k, query_label)\n",
    "\n",
    "        Lx_full = torch.cumsum(imatch_full, dim=0)\n",
    "        Lx_top_k = torch.cumsum(imatch_top_k, dim=0)\n",
    "\n",
    "        Px = Lx_full.float() / torch.arange(1, len(imatch_full)+1, 1).to(Lx_full)\n",
    "        rel = torch.sum(imatch_full)  # number of relevant items\n",
    "        ranking = Px * imatch_full  # this is to obtain the score of the matched item\n",
    "        AP = ranking.sum() / rel.clamp(min=1)  # clamp is to avoid division by zero if no relevant item retrieved\n",
    "\n",
    "        Px_top_k = Lx_top_k.float() / torch.arange(1, len(imatch_top_k)+1, 1).to(Lx_top_k)\n",
    "        rel_top_k = torch.sum(imatch_top_k)\n",
    "        ranking_top_k = Px_top_k * imatch_top_k\n",
    "        AP_top_k = ranking_top_k.sum() / rel_top_k.clamp(min=1)  # Average precision for top_k\n",
    "        \n",
    "        Lx_for_recall = (Lx_full >= 1).float()\n",
    "        # Lx_for_recall_top_k = (Lx_top_k >= 1).float()\n",
    "\n",
    "        for R in Rs:\n",
    "            rel_R = torch.sum(imatch_full[:R])\n",
    "            recalls[R].append(Lx_for_recall[R - 1])\n",
    "            precisions[R].append(rel_R / R)\n",
    "        \n",
    "        APs.append(AP)\n",
    "        AP_top_ks.append(AP_top_k)\n",
    "\n",
    "    APs = torch.tensor(APs)\n",
    "    AP_top_ks = torch.tensor(AP_top_ks)\n",
    "\n",
    "    recalls = {R: torch.tensor(recalls[R]) for R in Rs}\n",
    "    precisions = {R: torch.tensor(precisions[R]) for R in Rs}\n",
    "\n",
    "    mean_ap = APs.mean()\n",
    "    mean_ap_top_k = AP_top_ks.mean()\n",
    "    mean_recalls = {R: recalls[R].mean() for R in Rs}\n",
    "    mean_precisions = {R: precisions[R].mean() for R in Rs}\n",
    "\n",
    "    # Saving results to a file\n",
    "    with open(result_file_path, 'w') as file:\n",
    "        file.write(f\"Retrieval results for feature size {features_size}\\n\\n\")\n",
    "\n",
    "        file.write(f'Mean Average Precision full:\\n')\n",
    "        file.write(f'{mean_ap.item()}\\n\\n')\n",
    "\n",
    "        file.write(f'Mean Average Precision at top {top_k} (mAP@top_{top_k}):\\n')\n",
    "        file.write(f'{mean_ap_top_k.item()}\\n\\n')\n",
    "\n",
    "        file.write('Recall:\\n')\n",
    "        for R in mean_recalls:\n",
    "            file.write(f'R@{R}: {mean_recalls[R].item()}\\n')\n",
    "\n",
    "        file.write('\\nPrecision:\\n')\n",
    "        for R in mean_precisions:\n",
    "            file.write(f'P@{R}: {mean_precisions[R].item()}\\n')\n",
    "\n",
    "\n",
    "    print(f'\\nMean Average Precision full')\n",
    "    print(mean_ap)\n",
    "\n",
    "    print(f'\\nMean Average Precision top_k {top_k}')\n",
    "    print(mean_ap_top_k)\n",
    "\n",
    "    print('Recall')\n",
    "    for R in Rs:\n",
    "        print(f'R@{R}', recalls[R].mean())\n",
    "        \n",
    "    print('Precision')\n",
    "    for R in Rs:\n",
    "        print(f'P@{R}', precisions[R].mean())\n",
    "\n",
    "\n",
    "    return mean_ap, mean_ap_top_k, mean_recalls, mean_precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loaders.cub200loader import DataLoaderCUB200\n",
    "from helpers.load_models import load_resnet50_convV2, load_resnet50_unmodifiedVanilla\n",
    "from retrieval.run_retrieval_evaluation_baselines import run_retrieval_evaluation_baselines_models, run_retrieval_evaluation_Vanilla\n",
    "\n",
    "\n",
    "#### get data #####root, batch_size=32,num_workers=10   \n",
    "dataloadercub200 = DataLoaderCUB200(data_root, batch_size=batch_size, num_workers=10)\n",
    "_, testloader_cub200 = dataloadercub200.get_dataloaders()\n",
    "num_classes_cub200, _, label_to_name_test = dataloadercub200.get_number_of_classes()\n",
    "_, test_image_ids = dataloadercub200.get_unique_ids()\n",
    "# # ##############################################################################\n",
    "dir_retrieval = \"/home/alabutaleb/Desktop/confirmation/Retrieval_eval_baselines_experiment_gpu_0/retrieval_results_baselines_plus_vanilla\"\n",
    "\n",
    "dataset_names=\"cub200\"\n",
    "batch_size = 256\n",
    "lr = 7e-05\n",
    "top_k = 1000\n",
    "# feature_size = \"vanilla\"\n",
    "\n",
    "# vanilla finetuned resnet50\n",
    "feature_size_unmodifed = 2048\n",
    "vanilla_model = load_resnet50_unmodifiedVanilla(num_classes_cub200, feature_size_unmodifed, dataset_name, batch_size, lr, load_dir_vanilla)\n",
    "results_vanilla =  run_retrieval_evaluation_Vanilla(vanilla_model, testloader_cub200, dir_retrieval, batch_size, top_k, device)\n",
    "\n",
    "print_result_dict(results_vanilla, \"vanilla\")\n",
    "save_retrieval_results(results_vanilla, f\"{dir_retrieval}/results_dic_vanilla_resnet50.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
